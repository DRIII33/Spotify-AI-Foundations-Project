{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wau2pWtpY6PI",
        "outputId": "750d6436-e8ef-4a49-fdc5-4e4eb0ab3aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data loaded successfully.\n",
            "\n",
            "--- Causal Impact Simulation: T-Test Analysis ---\n",
            "Average stream duration for AI-exposed users: 134.66 seconds\n",
            "Average stream duration for non-AI users: 134.16 seconds\n",
            "\n",
            "Independent T-test Results:\n",
            "T-statistic: 2.0628\n",
            "P-value: 0.0391\n",
            "Conclusion: The p-value (0.0391) is less than the significance level (0.05).\n",
            "This suggests a statistically significant difference in stream duration between the two user groups.\n",
            "The new 'GenShuffle' feature likely had a measurable impact on user engagement.\n",
            "\n",
            "--- LLM-as-a-Judge Simulation: Comparing Human vs. AI Ratings ---\n",
            "Pearson Correlation between Human and LLM ratings: -0.9682\n",
            "Conclusion: A low correlation (-0.9682) suggests a significant disconnect between the LLM and human judgment.\n",
            "We should not use the LLM as a judge until its rating methodology is improved.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for data manipulation and causal inference.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Rationale: This notebook simulates a causal inference study to evaluate the impact of the new \"GenShuffle\" feature.\n",
        "# Since we cannot run a perfect A/B test on all users, this method allows us to approximate the effect\n",
        "# of the new feature by comparing a \"treatment\" group (users exposed to the AI)\n",
        "# with a similar \"control\" group (users not yet exposed).\n",
        "\n",
        "# --- Section 1: Data Loading and Preprocessing ---\n",
        "# We will use the same mock data files generated in Phase 1 to ensure a consistent analysis.\n",
        "try:\n",
        "    user_behavior_df = pd.read_csv('user_behavior.csv')\n",
        "    qualitative_ratings_df = pd.read_csv('qualitative_ratings.csv')\n",
        "    print(\"✅ Data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Please ensure 'user_behavior.csv' and 'qualitative_ratings.csv' are uploaded to the Colab environment.\")\n",
        "    raise\n",
        "\n",
        "# --- Section 2: Simulating Causal Analysis with T-Test ---\n",
        "# Method: We will identify a group of users who were \"treated\" (exposed to the AI feature)\n",
        "# and a control group. We then compare the average stream duration between these two groups\n",
        "# using a statistical test (a t-test).\n",
        "\n",
        "# Let's assume users with an even-numbered ID were a test group for the AI feature.\n",
        "# This simulates a feature rollout to a specific segment of users.\n",
        "user_behavior_df['exposed_to_ai_feature'] = user_behavior_df['user_id'].apply(lambda x: 1 if int(x.split('_')[1]) % 2 == 0 else 0)\n",
        "\n",
        "# Isolate the stream durations for the control and treatment groups.\n",
        "treatment_group_durations = user_behavior_df[user_behavior_df['exposed_to_ai_feature'] == 1]['stream_duration_sec']\n",
        "control_group_durations = user_behavior_df[user_behavior_df['exposed_to_ai_feature'] == 0]['stream_duration_sec']\n",
        "\n",
        "print(\"\\n--- Causal Impact Simulation: T-Test Analysis ---\")\n",
        "print(f\"Average stream duration for AI-exposed users: {treatment_group_durations.mean():.2f} seconds\")\n",
        "print(f\"Average stream duration for non-AI users: {control_group_durations.mean():.2f} seconds\")\n",
        "\n",
        "# Perform an independent t-test to check for a statistically significant difference.\n",
        "t_stat, p_value = stats.ttest_ind(treatment_group_durations, control_group_durations, equal_var=False)\n",
        "\n",
        "print(f\"\\nIndependent T-test Results:\")\n",
        "print(f\"T-statistic: {t_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation for stakeholders:\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f\"Conclusion: The p-value ({p_value:.4f}) is less than the significance level (0.05).\")\n",
        "    print(\"This suggests a statistically significant difference in stream duration between the two user groups.\")\n",
        "    print(\"The new 'GenShuffle' feature likely had a measurable impact on user engagement.\")\n",
        "else:\n",
        "    print(f\"Conclusion: The p-value ({p_value:.4f}) is greater than the significance level (0.05).\")\n",
        "    print(\"There is no statistically significant evidence to conclude a difference in stream duration.\")\n",
        "    print(\"Further analysis or a longer test period may be needed to detect an impact.\")\n",
        "\n",
        "# --- Section 3: LLM-as-a-Judge Evaluation Simulation ---\n",
        "# Method: We will use the 'qualitative_ratings' data to simulate a common technique in AI evaluation.\n",
        "# We'll compare the simulated ratings of a human expert with a Large Language Model used as a judge.\n",
        "# The goal is to see if our LLM judge's ratings are consistent with human judgment.\n",
        "\n",
        "print(\"\\n--- LLM-as-a-Judge Simulation: Comparing Human vs. AI Ratings ---\")\n",
        "\n",
        "# We will measure the consistency using a simple correlation coefficient.\n",
        "correlation, _ = stats.pearsonr(qualitative_ratings_df['human_coherence_rating'], qualitative_ratings_df['llm_coherence_rating'])\n",
        "\n",
        "print(f\"Pearson Correlation between Human and LLM ratings: {correlation:.4f}\")\n",
        "\n",
        "# Interpretation for stakeholders:\n",
        "if correlation > 0.8:\n",
        "    print(f\"Conclusion: A high correlation ({correlation:.4f}) indicates that the LLM judge is highly consistent with human ratings.\")\n",
        "    print(\"This suggests that we can confidently use this LLM for automated quality assessment of commentary.\")\n",
        "elif correlation > 0.5:\n",
        "    print(f\"Conclusion: A moderate correlation ({correlation:.4f}) suggests some alignment, but the LLM judge may require further tuning.\")\n",
        "else:\n",
        "    print(f\"Conclusion: A low correlation ({correlation:.4f}) suggests a significant disconnect between the LLM and human judgment.\")\n",
        "    print(\"We should not use the LLM as a judge until its rating methodology is improved.\")"
      ]
    }
  ]
}