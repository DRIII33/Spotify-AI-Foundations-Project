{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2xnIDLhe0qK",
        "outputId": "582d88f7-f2f0-4fe3-8510-733404e615fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully generated 'user_behavior.csv' with 250,000 user interaction logs.\n",
            "✅ Successfully generated 'gen_shuffle_events.csv' with 50,000 AI feature events.\n",
            "✅ Successfully generated 'qualitative_ratings.csv' for AI model quality assessment.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for data generation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# --- Rationale for data generation ---\n",
        "# The goal is to create a realistic simulation of Spotify's data ecosystem for a new AI feature.\n",
        "# We are generating three key datasets to represent:\n",
        "# 1. 'user_behavior.csv': Standard user interaction logs (streams, skips).\n",
        "# 2. 'gen_shuffle_events.csv': Logs specific to the new generative AI feature, including commentary text and user feedback.\n",
        "# 3. 'qualitative_ratings.csv': A small, curated dataset to simulate a \"human-in-the-loop\" or \"LLM-as-a-judge\" evaluation of the AI's output.\n",
        "# This approach allows us to demonstrate a comprehensive evaluation strategy.\n",
        "\n",
        "# Define parameters for the simulation to ensure consistency and scale.\n",
        "NUM_USERS = 1000\n",
        "NUM_TRACKS = 5000\n",
        "NUM_PLAYLISTS = 200\n",
        "NUM_DAYS = 30\n",
        "START_DATE = datetime(2025, 7, 1)\n",
        "\n",
        "# Generate unique identifiers for our simulated entities.\n",
        "users = [f'user_{i:04d}' for i in range(NUM_USERS)]\n",
        "tracks = [f'track_{i:05d}' for i in range(NUM_TRACKS)]\n",
        "playlists = [f'playlist_{i:03d}' for i in range(NUM_PLAYLISTS)]\n",
        "\n",
        "# --- Generating the `user_behavior` dataset ---\n",
        "# This dataset captures core user interactions with music. It's the baseline for our analysis.\n",
        "user_behavior_data = []\n",
        "for _ in range(250000): # Simulating a large number of user events for a realistic scale.\n",
        "    user_id = random.choice(users)\n",
        "    playlist_id = random.choice(playlists)\n",
        "    track_id = random.choice(tracks)\n",
        "\n",
        "    # Assign a random timestamp within our simulated 30-day period.\n",
        "    timestamp = START_DATE + timedelta(days=random.randint(0, NUM_DAYS-1),\n",
        "                                        hours=random.randint(0, 23),\n",
        "                                        minutes=random.randint(0, 59))\n",
        "\n",
        "    # Simulate realistic stream durations and a skip rate, which are key metrics.\n",
        "    stream_duration = np.random.randint(30, 240)\n",
        "    skip = np.random.choice([0, 1], p=[0.7, 0.3]) # 30% skip rate is a common industry benchmark.\n",
        "\n",
        "    # Event type for clarity and future analysis.\n",
        "    event_type = 'stream_play'\n",
        "\n",
        "    user_behavior_data.append([user_id, playlist_id, track_id, timestamp, stream_duration, skip, event_type])\n",
        "\n",
        "# Convert the list of data into a pandas DataFrame.\n",
        "user_behavior_df = pd.DataFrame(user_behavior_data, columns=['user_id', 'playlist_id', 'track_id', 'event_timestamp', 'stream_duration_sec', 'skipped_flag', 'event_type'])\n",
        "user_behavior_df.to_csv('user_behavior.csv', index=False)\n",
        "print(\"✅ Successfully generated 'user_behavior.csv' with 250,000 user interaction logs.\")\n",
        "\n",
        "# --- Generating the `gen_shuffle_events` dataset ---\n",
        "# This dataset specifically logs events related to the new AI-powered feature.\n",
        "gen_shuffle_events_data = []\n",
        "# Pre-defined commentary templates to simulate the AI's output.\n",
        "commentary_templates = [\n",
        "    \"Next up, we're shifting gears from that indie rock vibe to a classic throwback. This one's a fan favorite.\",\n",
        "    \"Following that high-energy track, I've got something to help you wind down. Perfect for a chill night in.\",\n",
        "    \"Did you know this artist's first single was a viral sensation? Here's the track that put them on the map.\",\n",
        "    \"Based on your recent listening, I'm predicting you'll love this song. Let's see if I'm right!\",\n",
        "    \"This track features some unique production that you might have missed. Listen for the subtle synth line in the chorus.\"\n",
        "]\n",
        "\n",
        "for _ in range(50000): # Simulating a large number of AI-driven events.\n",
        "    user_id = random.choice(users)\n",
        "    timestamp = START_DATE + timedelta(days=random.randint(0, NUM_DAYS-1),\n",
        "                                        hours=random.randint(0, 23),\n",
        "                                        minutes=random.randint(0, 59))\n",
        "\n",
        "    # We are simulating different event types to capture various user interactions.\n",
        "    event_type = random.choice(['commentary_start', 'user_feedback_positive', 'user_feedback_negative'])\n",
        "\n",
        "    commentary_text = \"\"\n",
        "    if event_type == 'commentary_start':\n",
        "        commentary_text = random.choice(commentary_templates)\n",
        "\n",
        "    gen_shuffle_events_data.append([user_id, timestamp, event_type, commentary_text])\n",
        "\n",
        "gen_shuffle_events_df = pd.DataFrame(gen_shuffle_events_data, columns=['user_id', 'event_timestamp', 'event_type', 'ai_commentary_text'])\n",
        "gen_shuffle_events_df.to_csv('gen_shuffle_events.csv', index=False)\n",
        "print(\"✅ Successfully generated 'gen_shuffle_events.csv' with 50,000 AI feature events.\")\n",
        "\n",
        "# --- Generating the `qualitative_ratings` dataset ---\n",
        "# This dataset is crucial for the qualitative evaluation of the AI model.\n",
        "qualitative_ratings_data = []\n",
        "# A small set of sample commentaries, including some that might be flagged for quality issues.\n",
        "sample_commentaries = random.sample(commentary_templates, 3) + [\"This is a test of the emergency broadcast system.\", \"The artist's name is actually spelled with a 'Y', not an 'I'.\"]\n",
        "for commentary in sample_commentaries:\n",
        "    # Simulating ratings from a human rater and an LLM-as-a-judge.\n",
        "    human_coherence = np.random.choice([1, 2, 3, 4, 5])\n",
        "    llm_coherence = np.random.choice([1, 2, 3, 4, 5])\n",
        "    safety_flag = np.random.choice([0, 1], p=[0.95, 0.05]) # Simulate a low rate of safety issues.\n",
        "\n",
        "    qualitative_ratings_data.append([commentary, human_coherence, llm_coherence, safety_flag])\n",
        "\n",
        "qualitative_ratings_df = pd.DataFrame(qualitative_ratings_data, columns=['commentary_text', 'human_coherence_rating', 'llm_coherence_rating', 'safety_flag'])\n",
        "qualitative_ratings_df.to_csv('qualitative_ratings.csv', index=False)\n",
        "print(\"✅ Successfully generated 'qualitative_ratings.csv' for AI model quality assessment.\")"
      ]
    }
  ]
}